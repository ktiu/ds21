# Web scraping

## Lernziele dieser Sitzung

Sie können...

- HTML in seiner Grundstruktur interpretieren.
- gezielt einzelne Elemente einer Seite mit R auslesen.

## Vorbereitung

Für diese Lektion werden folgende Pakete benötigt:

```{r setuplocal, message=F}
library(tidyverse)
library(rvest)
```

## Exkurs: HTML

Wenn man eine Webseite ganz normal in einem Browser aufruft, erscheint sie als eine Mischung aus formatiertem Text, Bildern, Designelementen, ggf. Videos, usw. Was aber im Hintergrund eigentlich vom Server an den Browser übertragen wird, ist eine Textdatei in einem bestimmten Format -- HTML (Hyptertext Markup Language). Darin wird Text auf eine genau festgelegte Art und Weise annotiert, damit der Browser weiß, wie er ihn anzeigen soll. Im HTML-Dokument kann auch stehen: Lade ein Bild von einer bestimmten Stelle und zeig es an dieser Stelle an.

Einen brauchbaren Überblick über die HTML-Elemente und die Struktur einer HTML-Datei gibt es hier: https://www.tutorialspoint.com/de/html/

An dieser Stelle ist wichtig ist zu wissen: HTML-Elemente ("Tags" oder "Nodes") sind streng hierarchisch angeordnet. Sie bestehen oft aus einem Anfangs- und einem End-Tag in spitzen Klammern:

```
<html>
  <head>
    <title>Titel meiner Webseite</title>
  </head>
  <body>
    <h1>Überschrift</h1>
    <p>Erster Absatz mit <b>fettem Text</b></p>
    <p>Zweiter Absatz mit <i>kursivem Text</i></p>
    <img src="path/to/image.jpg alt="Ein Bild" />
  </body>
</html>
```

In diesem Beispiel ist das Bild mit `<img />` das einzige Element, das nicht geöffnet und wieder geschlossen wird. Außerdem hat dieses Element Attribute (`src` und `alt`) mit bestimmten Werten. Eine echte Webseite ist weitaus komplexer und unübersichtlicher.

Dem Browser kann man sagen: Zeig mir nicht wie üblich die „gerenderte“ Seite an, sondern die zu Grunde liegende HTML-Datei. Das geht mit „Quelltext anzeigen“ / „View Source“ o.ä.

Viele Browser (hier seien Chrome und Firefox empfohlen) haben auch einen Modus namens „Entwicklertools“ / „Developer tools“, in dem die HTML-Elemente hierarchisch geordnet sind.

## Seite laden

Beim so genannten Web Scraping ist die Grundidee, dass wir eine Webseite nicht im Browser öffnen, sondern den HTML-Quelltext direkt in R laden. R kann dann aus dem Quelltext bestimmte Elemente extrahieren.

In der letzten Sitzung haben wir schon gesehen, wie Tabellen nach genau diesem Prinzip von einer Webseite direkt in R geladen werden können. Jetzt soll es darum gehen, noch präziser zu sagen, welche Elemente wir von einer bestimmten Webseite ziehen wollen.

Als Beispiel soll die Infoseite eines Wohnheims des Studentenwerks Frankfurt dienen. Die Adresse ist: https://www.studentenwerkfrankfurt.de/wohnen/wohnheime/frankfurt-am-main/kleine-seestrasse-11

Zunächst laden wir den Quelltext in R und nennen ihn `quelltext`:

```{r quelltext}
quelltext <- read_html("https://www.studentenwerkfrankfurt.de/wohnen/wohnheime/frankfurt-am-main/kleine-seestrasse-11")

quelltext
```

In diesem Schritt hat R den HTML-Quelltext schon "geparsed", d.h. ihn nicht nur als Text gespeichert, sondern als hierarchische Konstruktion mit den beiden Grundelementen `head` und `html`.

## Elemente suchen

Uns soll jetzt das Baujahr interessieren. Auf der Seite stehen die Informationen rechts neben dem Bild. Mit den Entwicklertools können wir schauen, wie die Elemente in HTML genau heißen. Ein geeigneter Ausgangspunkt wäre das `<div>`-Element mit dem Attribut `id="c599"`.

In R können wir dieses einzelne Element ansprechen mit:

```{r}
quelltext %>%
  html_node("div#c599")
```

Dann gehen wir in der Hierarchie drei `<div>`- Elemente „tiefer“. (`<div>`- Elemente sind abstrakte Container und werden im Webdesign oft angewendet.)

```{r}
quelltext %>%
  html_node("div#c599") %>%
  html_node("div") %>%
  html_node("div")
```

Alternativ könnten wir auch sagen: Darin das `div` mit `class="frame-inner"`:

```{r}
quelltext %>%
  html_node("div#c599") %>%
  html_node("div.frame-inner")
```

mit `html_nodes()` (Mehrzahl) werden alle Unterelemente eines Typs (hier `<p>` = Paragraph) angesprochen. Davon dann den Textinhalt (`html_text()`) gibt uns die relevanten Informationen:

```{r}
quelltext %>%
  html_node("div#c599") %>%
  html_node("div.frame-inner") %>%
  html_nodes("p") %>%
  html_text()
```

## Elemente reinigen

Jetzt ließe sich der dritte Eintrag dieses Ergebnisvectors reinigen und als Ergebnis "speichern".

Der Befehl `str_extract()` wendet dabei einen Regulären Ausdruck an, der nach einer Folge von vier Zahlen sucht.

```{r}
quelltext %>%
  html_node("div#c599") %>%
  html_node("div.frame-inner") %>%
  html_nodes("p") %>%
  html_text() %>%
  .[3] %>%
  str_extract("[0-9]{4}") %>%
  as.numeric() -> baujahr

baujahr
```

Wie diese Technik automatisiert auf eine Reihe von Seiten angewendet werden kann, wird zu einem späteren Zeitpunkt besprochen.

## Aufgaben

1. Lesen Sie die Anzahl der Wohnheimplätze aus.

```{r}
quelltext
```

```{r echo=solutions, class.source='solution', out.extra='class="solution"'}
quelltext %>%
  html_node("div#c599") %>%
  html_node("div.frame-inner") %>%
  html_nodes("p") %>%
  html_text() %>%
  .[2] %>%
  str_extract("[0-9]+") %>%
  as.numeric() -> anzahl_plaetze

anzahl_plaetze
```

2. Lesen Sie Baujahr und Anzahl der Wohnheimplätze von einem anderen Wohnheim aus. Was muss angepasst werden?

```{r echo=solutions, class.source='solution', out.extra='class="solution"'}
# z.B.: https://www.studentenwerkfrankfurt.de/wohnen/wohnheime/wiesbaden/max-kade-haus-adolfsallee-49-53

"https://www.studentenwerkfrankfurt.de/wohnen/wohnheime/wiesbaden/max-kade-haus-adolfsallee-49-53" %>%
  read_html() -> quelltext_wi

# Baujahr nicht vorhanden!
baujahr_wi <- NA

quelltext_wi %>%
  html_node("div#c1563") %>%
  # Auf dieser Seite ist es eine andere div-ID!
  # Außerdem: Appartment = Platz? Scheint aber so zu sein...
  html_node("div.frame-inner") %>%
  html_nodes("p") %>%
  html_text() %>%
  .[2] %>%
  str_extract("[0-9]+") %>%
  as.numeric() -> anzahl_plaetze_wi

anzahl_plaetze_wi
```

3. Ändern Sie das Script so, dass es auf beiden (allen) Wohnheimseiten funktioniert.

```{r echo=solutions, class.source='solution', out.extra='class="solution"'}
# Hier kann (hoffentlich) auch jede andere Wohnheim-URL eingesetzt werden.
"https://www.studentenwerkfrankfurt.de/wohnen/wohnheime/frankfurt-am-main/kleine-seestrasse-11" %>%
  read_html() -> quelltext_x

quelltext_x %>%
  # So funktioniert es unabhänging von ID:
  html_node("div.wohnheim-2 > div:nth-child(2)") %>%
  html_nodes("p") %>%
  html_text() -> items

items[3] %>%
  str_extract("[0-9]{4}") %>%
  as.numeric() -> baujahr_x

items[2] %>%
  # Funktioniert leider nicht bei der getrennten Angabe mehrerer Kategorien:
  str_extract("[0-9]+") %>%
  as.numeric() -> anzahl_plaetze_x
```

4. Lesen Sie die Adresse eines Wohnheims aus. Speichern Sie dabei Straße, Hausnummer, Postleitzahl und Ort getrennt. 

```{r echo=solutions, class.source='solution', out.extra='class="solution"'}
# Wie in Aufgabe 5 ersichtlich lassen sich die beiden Adresszeilen eigentlich
# recht einfach aus der Übersichtsseite auslesen.

# Eine Schwierigkeit beim Auslesen aus der Einzelseite liegt darin, dass die
# Funktion html_text() den Zeilenumbruch <br> "verschluckt". Die neueste Version
# von rvest beinhaltet die Funktion html_text2(), die genau dieses Probem löst.

# Die neue Version kann installiert werden mit:
# install.packages("devtools")
# devtools::install_github("tidyverse/rvest")
# (Danach R neu starten)

quelltext_x %>%
  html_node("div.wohnheim-2 > div:nth-child(2)") %>%
  html_nodes("p") %>%
  html_text2() %>%
  .[1] %>%
  trimws() %>%
  str_split("\n") %>%
  .[[1]] -> adresse

# Die Hausnummer sind die Zahlen am Ende der ersten Zeile:
adresse[1] %>%
  str_extract("[-0-9]+$") -> hausnummer # Funktioniert aber nicht bei 1b u.ä...

# Die Straße ist der Rest:
adresse[1] %>%
  str_remove(" [0-9]+$") -> strasse

# Die PLZ sind die 5 Zahlen am Anfang der zweiten Zeile:
adresse[2] %>%
  str_extract("^[0-9]{5}") -> plz

# Der Ort ist der Rest:
adresse[2] %>%
  str_remove("^[0-9]{5} ") -> ort
```

5. Sammeln Sie eine Liste aller Wohnheime mit Link.

```{r echo=solutions, class.source='solution', out.extra='class="solution"'}
"https://www.studentenwerkfrankfurt.de/wohnen/wohnheime" %>%
  read_html() %>%
  html_nodes("div.thumbnail") -> items

items %>%
  html_node("a") %>%
  html_attr("href") -> links

items %>%
  html_node("h4") %>%
  html_text -> strasse_nr

items %>%
  html_node("p") %>%
  html_text() -> plz_ort

tibble(links, strasse_nr, plz_ort) -> index
```

6. Sammeln Sie einen Datensatz (tibble) aller "Nutzungsentgelte" mit Wohnheim, Baujahr, Anzahl Wohneinheiten und Adresse. Stellen Sie sich vor, es handelte sich um Tausende Wohnheime -- vermeiden Sie also Copy-Paste-Strategien.

```{r echo=solutions, message=FALSE, class.source='solution', out.extra='class="solution"'}
# Hierfür gibt es einige Strategien. Meine präferierte Variante erfordert
# zunächst eine eigene Funktion, die eine URL nimmt und die gewünschten
# Informationen ausgibt:

scrape <- function(url) {
  url %>%
    read_html -> quelltext

  quelltext %>%
    html_table() %>%
    last() %>%
    # Die Größe soll immer ein String sein, weil es sonst später Probeme gibt:
    mutate(`Größe m²` = as.character(`Größe m²`)) -> Nutzungsentgelte

  quelltext %>%
    html_node("div.wohnheim-2 > div:nth-child(2)") %>%
    html_nodes("p") -> items

  items[[1]] %>%
    html_text2() %>%
    trimws() %>%
    str_split("\n") %>%
    .[[1]] -> adresse
  
  Nutzungsentgelte$strasse <- adresse[1]
  Nutzungsentgelte$ort <- adresse[2]
     
  items[3] %>%
    html_text() %>%
    str_extract("[0-9]{4}") %>%
    as.numeric() -> Nutzungsentgelte$baujahr
  
  items[2] %>%
    html_text() %>%
    str_extract("[0-9]+") %>%
    as.numeric() -> Nutzungsentgelte$anzahl_plaetze
  
  return(Nutzungsentgelte)
}

# Dann kann ich die eigene Funktion "scrape" auf alle Links anwenden. Das
# Ergebnis ist eine Liste von Tibbles. Die lässt sich schließlich noch
# kombinieren:

index$links %>%
 paste0("https://www.studentenwerkfrankfurt.de", .) %>%
  map(scrape) %>%
  do.call(bind_rows, .) -> liste

liste
```
